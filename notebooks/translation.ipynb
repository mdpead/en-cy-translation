{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mdpead/mdpead.github.io/en-cy-translation/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import datasets\n",
    "import torch.nn as nn\n",
    "import tokenizers\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PERC = 0.8\n",
    "TEST_PERC = 0.2\n",
    "VOCAB_SIZE = 10000\n",
    "MODEL_DIM = 512\n",
    "MAX_LENGTH = 1024\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea behind machine translation is to take a piece of text in one language and translate it to another language. In this example, we will look at translating English text into Welsh text.\n",
    "\n",
    "In order to validate our approach, we need examples of English text that have been (correctly) translated to the corresponding Welsh."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 1453009/1453009 [00:00<00:00, 2076981.96 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ds = datasets.load_dataset(TRAINING_DS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text_en', 'text_cy'],\n",
       "        num_rows: 1453009\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_ds(ds):\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_en': [\"A Cardiff University lecturer who's thought to have taught more British newspaper journalists than any other has officially put down his red pen after 35 years at the helm of one the UK's longest-running and most successful journalism courses.\",\n",
       "  'Video Interaction Guidance (VIG) is an evidence-based, strengths focused intervention aimed at working with parents or carers to improve relationships within the family.',\n",
       "  'Yours sincerely',\n",
       "  'All with links to leading authorities.',\n",
       "  'Graduate Programmes In Audit With PwC - Spring 2022'],\n",
       " 'text_cy': [\"Mae un o ddarlithwyr Prifysgol Caerdydd, sydd yn ôl pob tebyg wedi addysgu mwy o newyddiadurwyr papur newydd ym Mhrydain nag unrhyw un arall, wedi rhoi ei feiro goch o'r neilltu yn swyddogol ar ôl 35 mlynedd wrth y llyw ar un o gyrsiau newyddiaduraeth mwyaf hirsefydlog a llwyddiannus y DU.\",\n",
       "  \"Mae Canllawiau Rhyngweithio trwy Fideo (VIG) yn ymyriad sy'n seiliedig ar dystiolaeth ac yn canolbwyntio ar gryfderau sy'n ceisio gweithio gyda rhieni neu ofalwyr i wella perthnasedd o fewn y teulu.\",\n",
       "  'Yn gywir',\n",
       "  'Y cyfan gyda dolenni i awdurdodau blaenllaw.',\n",
       "  'Rhaglenni Graddedig yn yr adran Archwilio gyda PwC - Gwanwyn 2022']}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model needs the following as input:\n",
    "1. The English text, converted into token indices\n",
    "2. The output Welsh text, converted into token indices\n",
    "3. The decoder input Welsh text, converted into token indices, which is a shifted version of the output.\n",
    "4. An attention mask for the English text\n",
    "5. An attention mask for the Welsh text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in processing the text is to break it up into separate tokens, and assign an index to each token. These are typically sub-word level pieces of text. We will do this separately for both English and Welsh, since they clearly have different atomic tokens.\n",
    "\n",
    "https://huggingface.co/course/chapter6/8?fw=pt#building-a-bpe-tokenizer-from-scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers import models, pre_tokenizers, trainers, processors\n",
    "from tokenizers import normalizers\n",
    "from tokenizers import decoders\n",
    "from transformers import PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(text):\n",
    "    tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
    "    tokenizer.normalizer = normalizers.Sequence(\n",
    "        [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]\n",
    "    )\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Sequence(\n",
    "        [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()]\n",
    "    )\n",
    "    special_tokens = [\"[BOS]\", \"[EOS]\", \"[PAD]\", \"[MASK]\", \"[UNK]\"]\n",
    "    tokenizer.model = models.WordPiece(unk_token=\"[UNK]\")\n",
    "    trainer = trainers.WordPieceTrainer(vocab_size=VOCAB_SIZE, special_tokens=special_tokens)\n",
    "    tokenizer.train_from_iterator(text, trainer)\n",
    "    tokenizer.post_processor = processors.TemplateProcessing(\n",
    "        single=\"[BOS] $A [EOS]\",\n",
    "        special_tokens=[\n",
    "            (\"[BOS]\", tokenizer.token_to_id(\"[BOS]\")),\n",
    "            (\"[EOS]\", tokenizer.token_to_id(\"[EOS]\")),\n",
    "        ],\n",
    "    )\n",
    "    tokenizer.decoder = decoders.WordPiece(prefix=\"##\")\n",
    "\n",
    "    pretrained_tokenizer = PreTrainedTokenizerFast(\n",
    "        tokenizer_object=tokenizer,\n",
    "        bos_token=\"[BOS]\",\n",
    "        eos_token=\"[EOS]\",\n",
    "        pad_token=\"[PAD]\",\n",
    "        mask_token=\"[MASK]\",\n",
    "        unk_token=\"[UNK]\",\n",
    "    )\n",
    "    return pretrained_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m english_tokenizer = create_tokenizer(\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     text=[\u001b[43mpair\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m pair \u001b[38;5;129;01min\u001b[39;00m ds[\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m      3\u001b[39m )\n\u001b[32m      4\u001b[39m welsh_tokenizer = create_tokenizer(\n\u001b[32m      5\u001b[39m     text=[pair[\u001b[32m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m pair \u001b[38;5;129;01min\u001b[39;00m ds[\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m      6\u001b[39m )\n",
      "\u001b[31mKeyError\u001b[39m: 0"
     ]
    }
   ],
   "source": [
    "english_tokenizer = create_tokenizer(\n",
    "    text=[pair[0] for pair in ds['train']]\n",
    ")\n",
    "welsh_tokenizer = create_tokenizer(\n",
    "    text=[pair[1] for pair in ds['train']]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'english_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m example_tokenizer_output = \u001b[43menglish_tokenizer\u001b[49m(\n\u001b[32m      2\u001b[39m     text=[ex[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m ex \u001b[38;5;129;01min\u001b[39;00m example_text],\n\u001b[32m      3\u001b[39m     return_token_type_ids=\u001b[38;5;28;01mFalse\u001b[39;00m, padding=\u001b[38;5;28;01mTrue\u001b[39;00m, truncation=\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m, return_attention_mask=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m      4\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'english_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "example_tokenizer_output = english_tokenizer(\n",
    "    text=[ex[0] for ex in example_text],\n",
    "    return_token_type_ids=False, padding=True, truncation=True, return_tensors=\"pt\", return_attention_mask=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'example_tokenizer_output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mexample_tokenizer_output\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'example_tokenizer_output' is not defined"
     ]
    }
   ],
   "source": [
    "example_tokenizer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'english_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43menglish_tokenizer\u001b[49m.batch_decode(example_tokenizer_output[\u001b[33m'\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m'\u001b[39m], clean_up_tokenization_spaces=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'english_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "english_tokenizer.batch_decode(example_tokenizer_output['input_ids'], clean_up_tokenization_spaces=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'english_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43menglish_tokenizer\u001b[49m.convert_ids_to_tokens(example_tokenizer_output[\u001b[33m'\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m1\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'english_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "english_tokenizer.convert_ids_to_tokens(example_tokenizer_output['input_ids'][1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, we need to deal with batches of input, and for training the model it is more efficient. The input to the model is:\n",
    "1. All the English tokens\n",
    "2. All the Welsh tokens\n",
    "3. The source attention mask to tell it which source tokens to pay attention to\n",
    "4. The target attention mask to tell it which target tokens to pay attention to\n",
    "5. The decoder input ids, which are right-shifted target tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(texts):\n",
    "    src_batch = english_tokenizer(\n",
    "        text=[text[0] for text in texts],\n",
    "        return_token_type_ids=False,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_attention_mask=True,\n",
    "        max_length=MAX_LENGTH\n",
    "    )\n",
    "    src_batch = {'src_' + str(k): v for k,v in src_batch.items()}\n",
    "    tgt_batch = welsh_tokenizer(\n",
    "        text=[text[1] for text in texts],\n",
    "        return_token_type_ids=False,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_attention_mask=True,\n",
    "        max_length=MAX_LENGTH\n",
    "    )\n",
    "    tgt_batch = {'tgt_' + str(k): v for k,v in tgt_batch.items()}\n",
    "    tgt_batch['tgt_output_ids'] = tgt_batch['tgt_input_ids'][:, 1:]\n",
    "    tgt_batch['tgt_input_ids'] = tgt_batch['tgt_input_ids'][:, :-1]\n",
    "    tgt_batch['tgt_attention_mask'] = tgt_batch['tgt_attention_mask'][:, :-1]\n",
    "\n",
    "    # Extend shortest input\n",
    "    src_shape = src_batch['src_input_ids'].shape\n",
    "    tgt_shape = tgt_batch['tgt_input_ids'].shape\n",
    "    if src_shape[1] < tgt_shape[1]:\n",
    "        diff = tgt_shape[1] - src_shape[1]\n",
    "        src_batch['src_input_ids'] = torch.cat((src_batch['src_input_ids'], torch.full([src_shape[0],diff], 2)), dim=1)\n",
    "        src_batch['src_attention_mask'] = torch.cat((src_batch['src_attention_mask'], torch.full([src_shape[0],diff], 0)), dim=1)\n",
    "    elif tgt_shape[1] < src_shape[1]:\n",
    "        diff = src_shape[1] - tgt_shape[1]\n",
    "        tgt_batch['tgt_input_ids'] = torch.cat((tgt_batch['tgt_input_ids'], torch.full([tgt_shape[0],diff], 2)), dim=1)\n",
    "        tgt_batch['tgt_attention_mask'] = torch.cat((tgt_batch['tgt_attention_mask'], torch.full([tgt_shape[0],diff], 0)), dim=1)\n",
    "        tgt_batch['tgt_output_ids'] = torch.cat((tgt_batch['tgt_output_ids'], torch.full([tgt_shape[0],diff], 2)), dim=1)\n",
    "\n",
    "    # Combine\n",
    "    batch = {**src_batch, **tgt_batch}\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_batch = collate_batch(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src_input_ids': tensor([[   0,   45,  741,  131,  131,  167,  813, 2058,  112,  671, 8549,  435,\n",
       "          5596, 1245,   18,    1,    2,    2,    2,    2,    2,    2],\n",
       "         [   0, 4402, 1068,  250, 2665,  398,  202,  697, 2062,  768,   16,  136,\n",
       "           112, 7402,  170,  583, 9528,   83,  246, 1028,   18,    1]]),\n",
       " 'src_attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       " 'tgt_input_ids': tensor([[   0, 1070,   63,  195,  196,   53, 1577,   47, 1842,   11,   56, 9015,\n",
       "           583,  326,  682, 1735,   18,    1,    2,    2,    2,    2],\n",
       "         [   0, 2704, 1213, 2214,  432, 3405, 1984, 2936,   39, 7446,   80,  153,\n",
       "          2502,  132,  161,   53,  558,   18,    2,    2,    2,    2]]),\n",
       " 'tgt_attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]]),\n",
       " 'tgt_output_ids': tensor([[1070,   63,  195,  196,   53, 1577,   47, 1842,   11,   56, 9015,  583,\n",
       "           326,  682, 1735,   18,    1,    2,    2,    2,    2,    2],\n",
       "         [2704, 1213, 2214,  432, 3405, 1984, 2936,   39, 7446,   80,  153, 2502,\n",
       "           132,  161,   53,  558,   18,    1,    2,    2,    2,    2]])}"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the input that our model will receive, together with the outcome it will be trained against."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model we will build is a canonical encoder-decoder model. On the encoder side, a representation of the input is created, and on the decoder side this representation is used to generate a sequence of tokens in an autoregressive way."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder is a stack of N transformer layers, each of which is composed of a self-attention layer with a dense layer, including a residual connection. Let's start by defining the general Encoder:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An embedding takes a sequence of token ids and converts it into a sequence of n-dimensional vector representations of each token. Both the encoder and decoder have separate embeddings, since they deal with separate languages.\n",
    "\n",
    "We will use the pytorch Embedding [https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model=512, n_vocab=20000):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(n_vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this looks for our example batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-25.8897, -39.9070, -14.5269,  ..., -26.1514,  -6.6897,  -2.0937],\n",
       "         [ 12.9494, -15.0668,   2.8416,  ...,  -2.0395,  -0.1918, -26.9800],\n",
       "         [ -3.0216, -13.3112,  -7.1148,  ...,  14.4628,  10.6719,  -2.4404],\n",
       "         ...,\n",
       "         [ 33.0440,  -3.4465, -11.5646,  ...,  19.6917,  18.8510,  17.0128],\n",
       "         [ 33.0440,  -3.4465, -11.5646,  ...,  19.6917,  18.8510,  17.0128],\n",
       "         [ 33.0440,  -3.4465, -11.5646,  ...,  19.6917,  18.8510,  17.0128]],\n",
       "\n",
       "        [[-25.8897, -39.9070, -14.5269,  ..., -26.1514,  -6.6897,  -2.0937],\n",
       "         [ 19.2847,  13.0735, -11.0542,  ...,   3.8324,  -6.8153,  -7.6713],\n",
       "         [-33.6921, -20.2254,  35.9021,  ...,   7.8319,  13.2310,  24.9708],\n",
       "         ...,\n",
       "         [ 44.5540,  -6.1444,  23.3959,  ...,  24.8200,   4.2109,  23.9702],\n",
       "         [  5.9880,  -0.1729,  -4.2262,  ...,   0.6301, -14.1546,   8.1401],\n",
       "         [-12.7087,  -6.5566, -18.9570,  ..., -18.0415,  -3.4669,  22.9071]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_embedding_output = Embeddings()(example_batch\n",
    "['src_input_ids'])\n",
    "example_embedding_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 22, 512])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_embedding_output.size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have converted each token in the batch into a vector of dimension MODEL_DIM. The first rank of the tensor represents the batch no, the second is the token no and the third is the vector coefficient of the embedding."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positional Encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to tell the model which position each token is in, the token embeddings have to be augmented with position information. We will use a common method that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model=512, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)],\n",
    "                         requires_grad=False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-28.7663, -43.2300, -16.1410,  ..., -27.9460,  -7.4330,  -1.2152],\n",
       "         [ 15.3232, -16.1405,   4.0705,  ...,  -1.1549,  -0.2130, -28.8667],\n",
       "         [ -2.3471, -15.2526,  -6.8648,  ...,   0.0000,  11.8579,  -1.6004],\n",
       "         ...,\n",
       "         [ 36.8821,  -2.7309, -13.4025,  ...,  22.9908,  20.9478,   0.0000],\n",
       "         [ 37.7300,  -3.3760, -12.3726,  ...,  22.9908,  20.9479,  20.0142],\n",
       "         [ 37.6452,  -0.0000, -11.7530,  ...,  22.9908,  20.9480,  20.0142]],\n",
       "\n",
       "        [[-28.7663, -43.2300, -16.1410,  ..., -27.9460,  -7.4330,  -1.2152],\n",
       "         [ 22.3624,  15.1265, -11.3692,  ...,   5.3694,  -7.5725,  -7.4125],\n",
       "         [-36.4253, -22.9350,  40.9317,  ...,   9.8132,  14.7013,  28.8565],\n",
       "         ...,\n",
       "         [ 49.6710,  -5.7286,  25.4425,  ...,   0.0000,   4.6810,   0.0000],\n",
       "         [  7.6677,   0.2613,  -4.2188,  ...,   1.8112, -15.7251,  10.1557],\n",
       "         [-13.1912,  -7.8937, -19.9668,  ..., -18.9350,  -3.8497,  26.5634]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_pos_enc_output = PositionalEncoding()(example_embedding_output)\n",
    "example_pos_enc_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 22, 512])"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_pos_enc_output.size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the tensor is the same as before, since all that has been added is some positional information to each dimension of each token."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder Layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have transformed each input tokens into a vector representing that token, via an embedding, and added positional information, via the positional encoder. Now, the role of the encoder is to represent the input in such a way that the decoder can make best use of it. In order to do this, each layer of the encoder \"shares\" semantic information between tokens."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Headed Attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-headed attention is the mechanism by which this sharing takes place. It allows each token to learn how to \"pay attention\" to other tokens, and gradually assimilate their semantic meaning into their representations. Since the model has to deal with sequences of varying length, the attention mechanism has to learn how to first assess the relationship between tokens, and then how to share their representations i.e. it cannot say \"mix token 1 with token 3\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask.unsqueeze(2) == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        query, key, value = \\\n",
    "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))]\n",
    "\n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "        x, self.attn = attention(query, key, value, mask=mask,\n",
    "                                 dropout=self.dropout)\n",
    "\n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        x = x.transpose(1, 2).contiguous() \\\n",
    "             .view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the output of the multi-headed attention we need to feed it some input. We can feed it separate values for Q, K, and V. For the encoder, these will all be the representations of the tokens, hence the term \"self-attention\". We also input the attention mask, since we need to tell the attention layer to ignore any tokens that represent padding, coming from our batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   0,   45,  741,  131,  131,  167,  813, 2058,  112,  671, 8550,  435,\n",
       "         5596, 1245,   18,    1,    2,    2,    2,    2,    2,    2],\n",
       "        [   0, 4402, 1068,  250, 2665,  398,  202,  697, 2062,  768,   16,  136,\n",
       "          112, 7402,  170,  583, 9526,   91,  246, 1028,   18,    1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tokenizer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  0.1920,  -8.0616,  12.1870,  ...,   4.6376,  -6.3434,   3.2600],\n",
       "         [ 16.3910,  -4.1503,  17.4403,  ...,  -3.6862,  16.8644,  -7.1311],\n",
       "         [  9.2246,  -4.8245,  -5.3588,  ...,   3.9072,   2.9700,   2.0772],\n",
       "         ...,\n",
       "         [  0.5158,  -0.8326,   3.0285,  ...,   5.6848,  13.2035, -10.7891],\n",
       "         [-10.9216,   6.9987,  -1.1041,  ...,   2.6961,   3.1806,  -5.1459],\n",
       "         [  1.2143,   9.1400, -12.1723,  ...,  13.5675,  11.1429, -12.4582]],\n",
       "\n",
       "        [[ -1.4643,   2.9577,   4.0780,  ...,  -5.3677,  -0.9760,  -0.2644],\n",
       "         [ -5.8037,  -2.5812,  13.9444,  ...,   1.3678,  -7.1553,  -4.0912],\n",
       "         [ -1.2225,  -8.0255,  -7.0852,  ...,   1.7904,  16.1246,   0.4308],\n",
       "         ...,\n",
       "         [  6.9562,   2.9363,   2.8264,  ...,  -1.5850, -15.3895,  -8.1148],\n",
       "         [ 11.7618,  12.5986,   2.7443,  ...,  -7.7506, -12.2355,  17.5846],\n",
       "         [  7.8088,   0.5737,  -4.4561,  ...,   0.6292,   2.3043,  10.7107]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_attn_output = MultiHeadedAttention(h=8, d_model=MODEL_DIM, dropout=0.1)(\n",
    "    example_pos_enc_output,\n",
    "    example_pos_enc_output,\n",
    "    example_pos_enc_output,\n",
    "    example_batch['src_attention_mask']\n",
    ")\n",
    "example_attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 22, 512])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_attn_output.size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer Normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub-layer Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.8614e-01,  2.8400e+01, -1.6949e-01,  ..., -9.2039e+00,\n",
       "          -4.8522e+01, -5.2071e+01],\n",
       "         [-2.1234e+01, -1.1041e+01,  5.3760e-01,  ..., -1.0642e+01,\n",
       "          -3.7987e+01, -5.1724e+01],\n",
       "         [ 3.7165e+01, -2.9675e+00, -4.3879e+00,  ..., -7.9804e+00,\n",
       "          -3.5800e+01, -3.8595e+01],\n",
       "         ...,\n",
       "         [ 2.0313e+01,  1.9306e-01, -3.7872e+00,  ...,  9.4374e+00,\n",
       "          -1.0144e+01,  4.0170e+01],\n",
       "         [ 2.1145e+01, -1.7933e+01, -2.7651e+00,  ...,  9.4319e+00,\n",
       "          -1.0148e+01,  4.0140e+01],\n",
       "         [ 2.1054e+01, -1.9154e+01, -2.1684e+00,  ...,  9.4140e+00,\n",
       "          -1.0086e+01,  4.0146e+01]],\n",
       "\n",
       "        [[-3.8584e+01,  2.8407e+01, -4.5539e+01,  ..., -9.1606e+00,\n",
       "          -4.8317e+01, -1.0985e-01],\n",
       "         [ 5.3257e+01,  1.0627e+00,  2.0269e+01,  ..., -3.7417e+01,\n",
       "          -2.8004e+01, -1.0899e+01],\n",
       "         [-2.4751e+01,  1.5936e+01, -9.3505e+00,  ..., -3.4933e-02,\n",
       "           1.5455e+01,  2.8899e+01],\n",
       "         ...,\n",
       "         [-3.4146e+01, -2.0783e+01,  2.9192e+01,  ...,  1.1478e+01,\n",
       "          -9.8178e+00,  2.5062e-01],\n",
       "         [ 1.1633e-01,  1.9509e-01,  1.6337e+01,  ..., -3.8382e+01,\n",
       "          -2.3815e+01,  3.8182e+00],\n",
       "         [ 1.6288e-02, -1.5382e+01, -1.1025e-01,  ...,  5.4210e+01,\n",
       "           3.0578e+01, -8.9217e+00]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_attn_sublayer_output = SublayerConnection(size=MODEL_DIM, dropout=0.1)(\n",
    "    example_pos_enc_output,\n",
    "    lambda x: MultiHeadedAttention(h=8, d_model=MODEL_DIM, dropout=0.1)(x, x, x, example_batch['src_attention_mask'])\n",
    ")\n",
    "example_attn_sublayer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 22, 512])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_attn_sublayer_output.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Position-Wise Feed-forward Layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each token vector is then fed through an identical densely connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.1836e+00, -9.2368e+00, -4.3725e+00,  ...,  5.4393e+00,\n",
       "          -5.9997e-01, -2.5736e+00],\n",
       "         [-1.3680e+01, -6.1491e+00, -5.7365e-01,  ...,  8.4583e+00,\n",
       "          -2.4544e+00, -1.9194e+00],\n",
       "         [ 9.5345e+00,  2.9896e-02,  5.6210e+00,  ..., -2.4381e+00,\n",
       "           2.2146e+00,  9.3947e-01],\n",
       "         ...,\n",
       "         [ 7.2934e+00, -3.4986e+00, -7.3746e-01,  ...,  1.2000e+01,\n",
       "          -1.2271e+00,  8.5686e+00],\n",
       "         [ 6.2151e+00, -1.9886e-01, -1.5534e+00,  ...,  5.8545e+00,\n",
       "          -6.2957e+00,  2.4263e+00],\n",
       "         [ 9.1377e+00, -3.8672e+00, -2.5255e+00,  ...,  6.0578e+00,\n",
       "          -1.5369e+00,  4.7688e-01]],\n",
       "\n",
       "        [[ 6.6993e+00, -9.3848e+00, -5.5839e+00,  ...,  5.6225e+00,\n",
       "          -4.2399e-01,  1.0231e+00],\n",
       "         [-8.7006e+00, -6.3309e+00,  4.5514e-04,  ...,  3.8513e+00,\n",
       "           5.5180e+00, -8.3046e+00],\n",
       "         [ 5.8206e+00,  2.4432e-01, -1.8336e-01,  ..., -4.1959e+00,\n",
       "           6.0570e+00,  1.3539e+00],\n",
       "         ...,\n",
       "         [-3.9602e+00, -4.2134e+00, -4.9186e+00,  ...,  1.7242e+00,\n",
       "          -2.8475e+00,  2.1472e+00],\n",
       "         [-5.4976e+00, -1.8523e+00, -5.0123e+00,  ..., -5.6061e+00,\n",
       "           6.8314e+00, -3.3766e+00],\n",
       "         [-1.0045e+01,  1.6167e+00, -1.2630e+01,  ...,  4.4386e+00,\n",
       "           3.7640e+00, -6.0676e+00]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_ffn_output = PositionwiseFeedForward(d_model=MODEL_DIM, d_ff=2048, dropout=0.1)(\n",
    "    example_attn_sublayer_output\n",
    ")\n",
    "example_ffn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 22, 512])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_ffn_output.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is also wrapped in a residual connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -0.1032,  28.4999,   0.0797,  ...,  -9.2788, -48.4657, -52.7605],\n",
       "         [-21.2607, -10.9605,   0.9027,  ..., -10.7737, -38.0838, -52.2825],\n",
       "         [ 36.9496,  -3.2906,  -4.1375,  ...,  -7.4094, -35.9227, -38.6707],\n",
       "         ...,\n",
       "         [ 19.8028,   0.5374,  -3.5848,  ...,   9.6251, -10.3398,  40.1705],\n",
       "         [ 20.6546, -17.6744,  -2.5636,  ...,   9.5368, -10.2153,  39.4363],\n",
       "         [ 21.0540, -19.0612,  -1.9367,  ...,   9.5919, -10.2420,  39.5824]],\n",
       "\n",
       "        [[-38.9004,  28.5042, -45.5385,  ...,  -9.3385, -48.1295,  -0.7858],\n",
       "         [ 53.2567,   1.1497,  20.3706,  ..., -37.2545, -28.0045, -11.0479],\n",
       "         [-24.9864,  15.9362,  -9.3149,  ...,   0.3484,  15.3407,  28.4971],\n",
       "         ...,\n",
       "         [-34.1461, -20.8889,  29.4540,  ...,  11.4437,  -9.4731,   0.0800],\n",
       "         [ -0.1019,   0.3047,  16.0980,  ..., -38.8291, -24.0562,   3.8281],\n",
       "         [ -0.4796, -15.5397,  -0.2046,  ...,  54.3686,  30.7018,  -8.8193]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sublayer_ffn_output = SublayerConnection(size=MODEL_DIM, dropout=0.1)(\n",
    "    example_attn_sublayer_output,\n",
    "    lambda x: PositionwiseFeedForward(d_model=MODEL_DIM, d_ff=2048, dropout=0.1)(x)\n",
    ")\n",
    "example_sublayer_ffn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 22, 512])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sublayer_ffn_output.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder Layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! We've reached the end of our first encoder layer. Putting it all together each encoder layer is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the full encoder is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.4310e-02,  1.1522e+00,  2.8671e-02,  ..., -4.1057e-01,\n",
       "          -1.9842e+00, -2.1611e+00],\n",
       "         [-9.1315e-01, -4.9973e-01,  5.9216e-04,  ..., -5.2752e-01,\n",
       "          -1.5928e+00, -2.1544e+00],\n",
       "         [ 1.5835e+00, -8.1105e-02, -1.8206e-02,  ..., -2.4071e-01,\n",
       "          -1.3675e+00, -1.5165e+00],\n",
       "         ...,\n",
       "         [ 8.2074e-01, -3.5501e-02, -2.0296e-01,  ...,  3.3824e-01,\n",
       "          -4.5953e-01,  1.6582e+00],\n",
       "         [ 8.8238e-01, -7.9344e-01, -1.2690e-01,  ...,  3.6146e-01,\n",
       "          -4.5266e-01,  1.7036e+00],\n",
       "         [ 8.2586e-01, -8.6521e-01, -1.2931e-01,  ...,  3.3479e-01,\n",
       "          -4.5858e-01,  1.6509e+00]],\n",
       "\n",
       "        [[-1.5873e+00,  1.1978e+00, -1.8639e+00,  ..., -4.5616e-01,\n",
       "          -2.0127e+00, -4.8626e-02],\n",
       "         [ 2.2002e+00, -1.3161e-01,  8.7085e-01,  ..., -1.6729e+00,\n",
       "          -1.2538e+00, -5.0743e-01],\n",
       "         [-1.0651e+00,  6.1448e-01, -3.9876e-01,  ..., -5.0641e-02,\n",
       "           6.5514e-01,  1.2407e+00],\n",
       "         ...,\n",
       "         [-1.4755e+00, -9.7373e-01,  1.2339e+00,  ...,  4.7053e-01,\n",
       "          -4.3948e-01, -9.4088e-03],\n",
       "         [-4.1529e-02, -4.8608e-02,  7.3784e-01,  ..., -1.6356e+00,\n",
       "          -1.0104e+00,  1.3880e-01],\n",
       "         [-1.0013e-01, -7.7182e-01, -1.1201e-01,  ...,  2.1567e+00,\n",
       "           1.1850e+00, -4.4790e-01]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = copy.deepcopy\n",
    "attn = MultiHeadedAttention(h=2, d_model=MODEL_DIM)\n",
    "ff = PositionwiseFeedForward(d_model=MODEL_DIM, d_ff=2048, dropout=0.1)\n",
    "dropout = 0.1\n",
    "example_encoder_output = Encoder(\n",
    "    layer=EncoderLayer(\n",
    "        size=MODEL_DIM,\n",
    "        self_attn=c(attn),\n",
    "        feed_forward=c(ff),\n",
    "        dropout=0.1),\n",
    "    N=2)(example_pos_enc_output, example_batch['src_attention_mask'])\n",
    "example_encoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 22, 512])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_encoder_output.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input to the decoder is:\n",
    "1. The output of the encoder\n",
    "2. The previous tokens in the sequence up to that point\n",
    "3. The source mask showing which source tokens the decoder can pay attention to\n",
    "4. The target mask showing which target tokens the decoder can pay attention to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src_input_ids': tensor([[   0,   45,  741,  131,  131,  167,  813, 2058,  112,  671, 8550,  435,\n",
       "          5596, 1245,   18,    1,    2,    2,    2,    2,    2,    2],\n",
       "         [   0, 4402, 1068,  250, 2665,  398,  202,  697, 2062,  768,   16,  136,\n",
       "           112, 7402,  170,  583, 9526,   91,  246, 1028,   18,    1]]),\n",
       " 'src_attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       " 'tgt_input_ids': tensor([[   0, 1070,   63,  195,  196,   53, 1577,   47, 1842,   11,   56, 9015,\n",
       "           583,  326,  682, 1735,   18,    1,    2,    2,    2,    2],\n",
       "         [   0, 2704, 1213, 2214,  432, 3405, 1984, 2936,   39, 7446,   83,  153,\n",
       "          2502,  132,  161,   53,  558,   18,    2,    2,    2,    2]]),\n",
       " 'tgt_attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]]),\n",
       " 'tgt_output_ids': tensor([[1070,   63,  195,  196,   53, 1577,   47, 1842,   11,   56, 9015,  583,\n",
       "           326,  682, 1735,   18,    1,    2,    2,    2,    2,    2],\n",
       "         [2704, 1213, 2214,  432, 3405, 1984, 2936,   39, 7446,   83,  153, 2502,\n",
       "           132,  161,   53,  558,   18,    1,    2,    2,    2,    2]])}"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  5.8286,  -0.0000,   9.6820,  ...,  -9.0398, -13.3486,   8.4563],\n",
       "         [ 24.2495, -19.6370,  -2.5031,  ..., -32.5745, -12.0261, -14.8670],\n",
       "         [-19.6888,  13.1659,   2.4430,  ...,  15.8603, -32.6508,  28.8665],\n",
       "         ...,\n",
       "         [ -5.5493,  26.5495,  -5.5052,  ..., -14.4604,   3.0550,  14.3575],\n",
       "         [ -4.7014,  25.9044,  -4.4752,  ...,  -0.0000,   3.0551,  14.3575],\n",
       "         [ -4.7862,  24.8424,  -3.8557,  ..., -14.4604,   3.0553,  14.3575]],\n",
       "\n",
       "        [[  5.8286,  -1.4550,   9.6820,  ...,  -9.0398,  -0.0000,   8.4563],\n",
       "         [ -0.0000,   3.6290, -18.0780,  ...,  -3.6336,  22.4507,  10.6727],\n",
       "         [ -0.0000, -16.1577,   1.7718,  ...,   0.0000, -36.5178, -72.5394],\n",
       "         ...,\n",
       "         [ -5.5493,  26.5495,  -5.5052,  ..., -14.4604,   3.0550,  14.3575],\n",
       "         [ -4.7014,  25.9044,  -4.4752,  ..., -14.4604,   3.0551,  14.3575],\n",
       "         [ -4.7862,  24.8424,  -3.8557,  ..., -14.4604,   3.0553,  14.3575]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tgt_embedding_output = Embeddings(512, VOCAB_SIZE)(example_batch['tgt_input_ids'])\n",
    "example_tgt_pos_enc_output = PositionalEncoding(512,0.1)(example_tgt_embedding_output)\n",
    "example_tgt_pos_enc_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 22, 512])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tgt_pos_enc_output.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.4241e+00,  1.5115e+01, -7.5266e+00,  ..., -1.1657e+01,\n",
       "          -1.0247e+01, -9.8763e-03],\n",
       "         [-1.2680e+01,  8.4389e+00, -3.0016e+00,  ..., -1.1956e+01,\n",
       "          -2.4363e+00,  5.6326e+00],\n",
       "         [ 1.7149e-01,  9.6464e+00, -1.8841e+00,  ...,  6.1402e+00,\n",
       "           4.8300e+00,  7.2312e+00],\n",
       "         ...,\n",
       "         [-2.1939e+00, -1.1523e+01, -2.7049e+00,  ...,  1.7027e+00,\n",
       "           6.4457e+00, -8.6503e+00],\n",
       "         [-1.0129e+01, -1.4715e+01, -6.6962e+00,  ...,  5.7623e+00,\n",
       "           1.5422e+01, -1.1384e+01],\n",
       "         [-6.6961e+00, -2.2064e+00,  7.2900e-02,  ..., -8.6733e+00,\n",
       "           1.0138e+01, -4.9338e+00]],\n",
       "\n",
       "        [[-4.9196e-01,  1.0320e+01, -6.2101e+00,  ..., -4.4297e+00,\n",
       "          -6.3575e+00, -4.8989e+00],\n",
       "         [-1.0765e+01, -2.1929e-01, -2.9287e+00,  ..., -8.0981e+00,\n",
       "          -2.1598e+00,  4.1278e-01],\n",
       "         [-1.2973e+00, -4.7371e+00,  2.0007e+01,  ..., -7.0164e+00,\n",
       "           1.6256e+01,  1.0959e+01],\n",
       "         ...,\n",
       "         [ 3.3669e+00, -2.8233e+00,  1.1229e+01,  ..., -1.8797e+00,\n",
       "          -1.0227e+01,  7.6143e+00],\n",
       "         [ 9.0398e+00,  2.0866e+00,  8.8805e+00,  ...,  6.4222e-01,\n",
       "           3.7510e+00,  3.9653e+00],\n",
       "         [-7.3333e+00,  7.1571e+00, -2.8085e+00,  ..., -1.7938e+00,\n",
       "          -1.0153e+01, -1.2503e+01]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MultiHeadedAttention(h=8, d_model=512)(\n",
    "    example_tgt_pos_enc_output,\n",
    "    example_tgt_pos_enc_output,\n",
    "    example_tgt_pos_enc_output,\n",
    "    example_batch['tgt_attention_mask']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"Follow Figure 1 (right) for connections.\"\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        return self.sublayer[2](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 6.0497e+00, -1.6359e-02,  9.2981e+00,  ..., -8.9984e+00,\n",
       "          -1.3124e+01,  8.7872e+00],\n",
       "         [ 2.4777e+01, -2.0105e+01, -2.6817e+00,  ..., -3.2752e+01,\n",
       "          -1.1998e+01, -1.4635e+01],\n",
       "         [-1.9287e+01,  1.2844e+01,  2.7165e+00,  ...,  1.6053e+01,\n",
       "          -3.2761e+01,  2.9111e+01],\n",
       "         ...,\n",
       "         [-5.5654e+00,  2.6423e+01, -5.4091e+00,  ..., -1.4361e+01,\n",
       "           2.7273e+00,  1.4192e+01],\n",
       "         [-4.6789e+00,  2.5582e+01, -4.2727e+00,  ...,  2.1494e-01,\n",
       "           2.8010e+00,  1.4332e+01],\n",
       "         [-4.7660e+00,  2.4532e+01, -3.5117e+00,  ..., -1.4462e+01,\n",
       "           2.4417e+00,  1.4431e+01]],\n",
       "\n",
       "        [[ 6.7228e+00, -1.3901e+00,  9.5352e+00,  ..., -8.8095e+00,\n",
       "          -5.9263e-02,  8.7638e+00],\n",
       "         [-3.6100e-01,  3.1946e+00, -1.7741e+01,  ..., -3.3016e+00,\n",
       "           2.2021e+01,  1.0593e+01],\n",
       "         [ 8.5785e-01, -1.5989e+01,  2.0714e+00,  ..., -1.3277e-01,\n",
       "          -3.6230e+01, -7.2237e+01],\n",
       "         ...,\n",
       "         [-5.5369e+00,  2.6181e+01, -5.4415e+00,  ..., -1.4411e+01,\n",
       "           2.8685e+00,  1.3968e+01],\n",
       "         [-4.6218e+00,  2.5845e+01, -4.3660e+00,  ..., -1.4460e+01,\n",
       "           3.0827e+00,  1.4463e+01],\n",
       "         [-4.8163e+00,  2.4636e+01, -3.7125e+00,  ..., -1.4305e+01,\n",
       "           2.9915e+00,  1.4369e+01]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = copy.deepcopy\n",
    "attn = MultiHeadedAttention(h=8, d_model=512)\n",
    "ff = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\n",
    "position = PositionalEncoding(d_model=512, dropout=0.1)\n",
    "DecoderLayer(512, c(attn), c(attn), c(ff), dropout=0.1)(\n",
    "    x=example_tgt_pos_enc_output,\n",
    "    memory=example_encoder_output,\n",
    "    src_mask=example_batch['src_attention_mask'],\n",
    "    tgt_mask=example_batch['tgt_attention_mask']\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"Generic N layer decoder with masking.\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = copy.deepcopy\n",
    "attn = MultiHeadedAttention(h=8, d_model=512)\n",
    "ff = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\n",
    "position = PositionalEncoding(d_model=512, dropout=0.1)\n",
    "example_decoder_output = Decoder(\n",
    "    layer=DecoderLayer(512, c(attn), c(attn), c(ff), dropout=0.1),\n",
    "    N=2\n",
    ")(x=example_tgt_pos_enc_output,\n",
    "    memory=example_encoder_output,\n",
    "    src_mask=example_batch['src_attention_mask'],\n",
    "    tgt_mask=example_batch['tgt_attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2352,  0.0194,  0.3702,  ..., -0.3726, -0.5356,  0.3208],\n",
       "         [ 1.0193, -0.7917, -0.0595,  ..., -1.3356, -0.4542, -0.6223],\n",
       "         [-0.9125,  0.5169,  0.0792,  ...,  0.6712, -1.4291,  1.2254],\n",
       "         ...,\n",
       "         [-0.2761,  1.0254, -0.3166,  ..., -0.6078,  0.0334,  0.5175],\n",
       "         [-0.2166,  1.0071, -0.2656,  ..., -0.0140,  0.0710,  0.5321],\n",
       "         [-0.2678,  0.9575, -0.2353,  ..., -0.6234,  0.0465,  0.5276]],\n",
       "\n",
       "        [[ 0.2201, -0.0543,  0.3694,  ..., -0.4048,  0.0091,  0.3324],\n",
       "         [-0.0220,  0.1297, -0.6954,  ..., -0.1551,  0.9529,  0.4282],\n",
       "         [-0.0164, -0.7912,  0.0068,  ..., -0.1014, -1.6373, -3.1989],\n",
       "         ...,\n",
       "         [-0.2698,  1.0694, -0.2796,  ..., -0.6404,  0.0931,  0.5407],\n",
       "         [-0.2630,  1.0132, -0.2573,  ..., -0.6471,  0.0601,  0.5237],\n",
       "         [-0.2471,  1.0049, -0.2273,  ..., -0.6291,  0.0848,  0.5633]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 22, 512])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_decoder_output.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to take the output of the decoder and predict the token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"Define standard linear + softmax generation step.\"\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_generator_output = Generator(d_model=512, vocab=VOCAB_SIZE)(example_decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 22, 10000])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_generator_output.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the generator takes in the decoder output for each step, and maps the decoder vector representation for each token to a probability for *all* tokens in the sequence, not just the last one. For example, the first sequence, first token output log probabilities are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -9.7038,  -9.0570,  -9.3031,  ..., -10.0155,  -8.9369,  -8.6091],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_generator_output[0, 0, :]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most likely token is therefore the one with the highest probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6807)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_ind = example_generator_output[0, 0, :].argmax()\n",
    "vocab_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ymad'"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "welsh_tokenizer.decode(vocab_ind)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-Decoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting the full model together gives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture. Base for this and many\n",
    "    other models.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"Take in and process masked src and target sequences.\"\n",
    "        return self.decode(self.encode(src, src_mask), src_mask,\n",
    "                            tgt, tgt_mask)\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "\n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(src_vocab, tgt_vocab, N=6,\n",
    "               d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn),\n",
    "                             c(ff), dropout), N),\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
    "        Generator(d_model, tgt_vocab))\n",
    "\n",
    "    # This was important from their code.\n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform(p)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1307/320853634.py:20: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(p)\n"
     ]
    }
   ],
   "source": [
    "example_model = make_model(VOCAB_SIZE, VOCAB_SIZE, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1307/320853634.py:20: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(p)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/mdpead/mdpead.github.io/translation/translation.ipynb Cell 115\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mdpead/mdpead.github.io/translation/translation.ipynb#Y216sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m example_model_output \u001b[39m=\u001b[39m make_model(VOCAB_SIZE, VOCAB_SIZE, \u001b[39m2\u001b[39;49m)(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mdpead/mdpead.github.io/translation/translation.ipynb#Y216sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     example_batch[\u001b[39m'\u001b[39;49m\u001b[39msrc_input_ids\u001b[39;49m\u001b[39m'\u001b[39;49m], example_batch[\u001b[39m'\u001b[39;49m\u001b[39mtgt_input_ids\u001b[39;49m\u001b[39m'\u001b[39;49m], example_batch[\u001b[39m'\u001b[39;49m\u001b[39msrc_attention_mask\u001b[39;49m\u001b[39m'\u001b[39;49m], example_batch[\u001b[39m'\u001b[39;49m\u001b[39mtgt_attention_mask\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mdpead/mdpead.github.io/translation/translation.ipynb#Y216sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m example_model_output\n",
      "File \u001b[0;32m~/mdpead.github.io/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/mdpead/mdpead.github.io/translation/translation.ipynb Cell 115\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mdpead/mdpead.github.io/translation/translation.ipynb#Y216sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, src, tgt, src_mask, tgt_mask):\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mdpead/mdpead.github.io/translation/translation.ipynb#Y216sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mTake in and process masked src and target sequences.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mdpead/mdpead.github.io/translation/translation.ipynb#Y216sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecode(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode(src, src_mask), src_mask,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mdpead/mdpead.github.io/translation/translation.ipynb#Y216sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m                         tgt, tgt_mask)\n",
      "\u001b[1;32m/home/mdpead/mdpead.github.io/translation/translation.ipynb Cell 115\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mdpead/mdpead.github.io/translation/translation.ipynb#Y216sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode\u001b[39m(\u001b[39mself\u001b[39m, src, src_mask):\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mdpead/mdpead.github.io/translation/translation.ipynb#Y216sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msrc_embed(src), src_mask)\n",
      "File \u001b[0;32m~/mdpead.github.io/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mdpead.github.io/.venv/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/mdpead.github.io/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/mdpead/mdpead.github.io/translation/translation.ipynb Cell 115\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mdpead/mdpead.github.io/translation/translation.ipynb#Y216sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mdpead/mdpead.github.io/translation/translation.ipynb#Y216sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlut(x) \u001b[39m*\u001b[39m math\u001b[39m.\u001b[39msqrt(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_model)\n",
      "File \u001b[0;32m~/mdpead.github.io/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mdpead.github.io/.venv/lib/python3.11/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/mdpead.github.io/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2179\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2118\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"A simple lookup table that looks up embeddings in a fixed dictionary and size.\u001b[39;00m\n\u001b[1;32m   2119\u001b[0m \n\u001b[1;32m   2120\u001b[0m \u001b[39mThis module is often used to retrieve word embeddings using indices.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2175\u001b[0m \u001b[39m             [ 0.6262,  0.2438,  0.7471]]])\u001b[39;00m\n\u001b[1;32m   2176\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2178\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight):\n\u001b[0;32m-> 2179\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2180\u001b[0m         embedding,\n\u001b[1;32m   2181\u001b[0m         (\u001b[39minput\u001b[39;49m, weight),\n\u001b[1;32m   2182\u001b[0m         \u001b[39minput\u001b[39;49m,\n\u001b[1;32m   2183\u001b[0m         weight,\n\u001b[1;32m   2184\u001b[0m         padding_idx\u001b[39m=\u001b[39;49mpadding_idx,\n\u001b[1;32m   2185\u001b[0m         max_norm\u001b[39m=\u001b[39;49mmax_norm,\n\u001b[1;32m   2186\u001b[0m         norm_type\u001b[39m=\u001b[39;49mnorm_type,\n\u001b[1;32m   2187\u001b[0m         scale_grad_by_freq\u001b[39m=\u001b[39;49mscale_grad_by_freq,\n\u001b[1;32m   2188\u001b[0m         sparse\u001b[39m=\u001b[39;49msparse,\n\u001b[1;32m   2189\u001b[0m     )\n\u001b[1;32m   2190\u001b[0m \u001b[39mif\u001b[39;00m padding_idx \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2191\u001b[0m     \u001b[39mif\u001b[39;00m padding_idx \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/mdpead.github.io/.venv/lib/python3.11/site-packages/torch/overrides.py:1534\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m \u001b[39mif\u001b[39;00m _is_torch_function_mode_enabled():\n\u001b[1;32m   1531\u001b[0m     \u001b[39m# if we're here, the mode must be set to a TorchFunctionStackMode\u001b[39;00m\n\u001b[1;32m   1532\u001b[0m     \u001b[39m# this unsets it and calls directly into TorchFunctionStackMode's torch function\u001b[39;00m\n\u001b[1;32m   1533\u001b[0m     \u001b[39mwith\u001b[39;00m _pop_mode_temporarily() \u001b[39mas\u001b[39;00m mode:\n\u001b[0;32m-> 1534\u001b[0m         result \u001b[39m=\u001b[39m mode\u001b[39m.\u001b[39;49m__torch_function__(public_api, types, args, kwargs)\n\u001b[1;32m   1535\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[1;32m   1536\u001b[0m         \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/mdpead.github.io/.venv/lib/python3.11/site-packages/torch/utils/_device.py:62\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39mif\u001b[39;00m func \u001b[39min\u001b[39;00m _device_constructors() \u001b[39mand\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\n\u001b[0;32m---> 62\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/mdpead.github.io/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "example_model_output = make_model(VOCAB_SIZE, VOCAB_SIZE, 2)(\n",
    "    example_batch['src_input_ids'], example_batch['tgt_input_ids'], example_batch['src_attention_mask'], example_batch['tgt_attention_mask'])\n",
    "example_model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 22, 512])"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_model_output.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forward pass of the model doesn't use the Generator?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "#import GPUtil\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train the model we need to decide on a few training parameters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    \"Implement label smoothing.\"\n",
    "\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(reduction=\"sum\")\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, true_dist.clone().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    \"A simple loss compute and train function.\"\n",
    "\n",
    "    def __init__(self, generator, criterion):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def __call__(self, x, y, norm):\n",
    "        x = self.generator(x)\n",
    "        sloss = (\n",
    "            self.criterion(\n",
    "                x.contiguous().view(-1, x.size(-1)), y.contiguous().view(-1)\n",
    "            )\n",
    "            / norm\n",
    "        )\n",
    "        return sloss.data * norm, sloss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_optimizer = torch.optim.Adam(\n",
    "        example_model.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate(step, model_size, factor, warmup):\n",
    "    \"\"\"\n",
    "    we have to default the step to 1 for LambdaLR function\n",
    "    to avoid zero raising to negative power.\n",
    "    \"\"\"\n",
    "    if step == 0:\n",
    "        step = 1\n",
    "    return factor * (\n",
    "        model_size ** (-0.5) * min(step ** (-0.5), step * warmup ** (-1.5))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_lr_scheduler = LambdaLR(\n",
    "    optimizer=example_optimizer,\n",
    "    lr_lambda=lambda step: rate(\n",
    "        step, 512, factor=1, warmup=100\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyOptimizer(torch.optim.Optimizer):\n",
    "    def __init__(self):\n",
    "        self.param_groups = [{\"lr\": 0}]\n",
    "        None\n",
    "\n",
    "    def step(self):\n",
    "        None\n",
    "\n",
    "    def zero_grad(self, set_to_none=False):\n",
    "        None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyScheduler:\n",
    "    def step(self):\n",
    "        None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(corpus['train'], batch_size=64, collate_fn=collate_batch)\n",
    "val_dataloader = torch.utils.data.DataLoader(corpus['val'], batch_size=64, collate_fn=collate_batch)\n",
    "test_dataloader = torch.utils.data.DataLoader(corpus['test'], batch_size=64, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainState:\n",
    "    \"\"\"Track number of steps, examples, and tokens processed\"\"\"\n",
    "\n",
    "    step: int = 0  # Steps in the current epoch\n",
    "    accum_step: int = 0  # Number of gradient accumulation steps\n",
    "    samples: int = 0  # total # of examples used\n",
    "    tokens: int = 0  # total # of tokens processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(\n",
    "    data_iter,\n",
    "    model,\n",
    "    loss_compute,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    mode=\"train\",\n",
    "    accum_iter=1,\n",
    "    train_state=TrainState(),\n",
    "):\n",
    "    \"\"\"Train a single epoch\"\"\"\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "    n_accum = 0\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        out = model.forward(\n",
    "            batch['src_input_ids'],\n",
    "            batch['tgt_input_ids'],\n",
    "            batch['src_attention_mask'],\n",
    "            batch['tgt_attention_mask']\n",
    "        )\n",
    "        ntokens = (batch['tgt_output_ids'] != 2).data.sum()\n",
    "        loss, loss_node = loss_compute(out, batch['tgt_output_ids'], ntokens)\n",
    "        # loss_node = loss_node / accum_iter\n",
    "        if mode == \"train\" or mode == \"train+log\":\n",
    "            loss_node.backward()\n",
    "            train_state.step += 1\n",
    "            train_state.samples += batch['src_input_ids'].shape[0]\n",
    "            train_state.tokens += ntokens\n",
    "            if i % accum_iter == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                n_accum += 1\n",
    "                train_state.accum_step += 1\n",
    "            scheduler.step()\n",
    "\n",
    "        total_loss += loss\n",
    "        total_tokens += ntokens\n",
    "        tokens += ntokens\n",
    "        if i % 40 == 1 and (mode == \"train\" or mode == \"train+log\"):\n",
    "            lr = optimizer.param_groups[0][\"lr\"]\n",
    "            elapsed = time.time() - start\n",
    "            print(\n",
    "                (\n",
    "                    \"Epoch Step: %6d | Accumulation Step: %3d | Loss: %6.2f \"\n",
    "                    + \"| Tokens / Sec: %7.1f | Learning Rate: %6.1e\"\n",
    "                )\n",
    "                % (i, n_accum, loss / ntokens, tokens / elapsed, lr)\n",
    "            )\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "        del loss\n",
    "        del loss_node\n",
    "    return total_loss / total_tokens, train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/mdpead/mdpead.github.io/translation/translation.ipynb Cell 134\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mdpead/mdpead.github.io/translation/translation.ipynb#Y244sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m run_epoch(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mdpead/mdpead.github.io/translation/translation.ipynb#Y244sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     data_iter\u001b[39m=\u001b[39;49mtrain_dataloader,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mdpead/mdpead.github.io/translation/translation.ipynb#Y244sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     model\u001b[39m=\u001b[39;49mexample_model,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mdpead/mdpead.github.io/translation/translation.ipynb#Y244sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     loss_compute\u001b[39m=\u001b[39;49mSimpleLossCompute(example_model\u001b[39m.\u001b[39;49mgenerator, LabelSmoothing(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mdpead/mdpead.github.io/translation/translation.ipynb#Y244sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m         size\u001b[39m=\u001b[39;49mVOCAB_SIZE, padding_idx\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, smoothing\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mdpead/mdpead.github.io/translation/translation.ipynb#Y244sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     )),\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mdpead/mdpead.github.io/translation/translation.ipynb#Y244sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     optimizer\u001b[39m=\u001b[39;49mexample_optimizer,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mdpead/mdpead.github.io/translation/translation.ipynb#Y244sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     scheduler\u001b[39m=\u001b[39;49mexample_lr_scheduler,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mdpead/mdpead.github.io/translation/translation.ipynb#Y244sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mdpead/mdpead.github.io/translation/translation.ipynb#Y244sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m )\n",
      "\u001b[1;32m/home/mdpead/mdpead.github.io/translation/translation.ipynb Cell 134\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mdpead/mdpead.github.io/translation/translation.ipynb#Y244sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m n_accum \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mdpead/mdpead.github.io/translation/translation.ipynb#Y244sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(data_iter):\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mdpead/mdpead.github.io/translation/translation.ipynb#Y244sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     out \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mforward(\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mdpead/mdpead.github.io/translation/translation.ipynb#Y244sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m         batch[\u001b[39m'\u001b[39;49m\u001b[39msrc_input_ids\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mdpead/mdpead.github.io/translation/translation.ipynb#Y244sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m         batch[\u001b[39m'\u001b[39;49m\u001b[39mtgt_input_ids\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mdpead/mdpead.github.io/translation/translation.ipynb#Y244sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m         batch[\u001b[39m'\u001b[39;49m\u001b[39msrc_attention_mask\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mdpead/mdpead.github.io/translation/translation.ipynb#Y244sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m         batch[\u001b[39m'\u001b[39;49m\u001b[39mtgt_attention_mask\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mdpead/mdpead.github.io/translation/translation.ipynb#Y244sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mdpead/mdpead.github.io/translation/translation.ipynb#Y244sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m     ntokens \u001b[39m=\u001b[39m (batch[\u001b[39m'\u001b[39m\u001b[39mtgt_output_ids\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39msum()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mdpead/mdpead.github.io/translation/translation.ipynb#Y244sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m     loss, loss_node \u001b[39m=\u001b[39m loss_compute(out, batch[\u001b[39m'\u001b[39m\u001b[39mtgt_output_ids\u001b[39m\u001b[39m'\u001b[39m], ntokens)\n",
      "\u001b[1;32m/home/mdpead/mdpead.github.io/translation/translation.ipynb Cell 134\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mdpead/mdpead.github.io/translation/translation.ipynb#Y244sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, src, tgt, src_mask, tgt_mask):\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mdpead/mdpead.github.io/translation/translation.ipynb#Y244sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mTake in and process masked src and target sequences.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mdpead/mdpead.github.io/translation/translation.ipynb#Y244sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecode(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode(src, src_mask), src_mask,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mdpead/mdpead.github.io/translation/translation.ipynb#Y244sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m                         tgt, tgt_mask)\n",
      "\u001b[1;32m/home/mdpead/mdpead.github.io/translation/translation.ipynb Cell 134\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mdpead/mdpead.github.io/translation/translation.ipynb#Y244sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode\u001b[39m(\u001b[39mself\u001b[39m, src, src_mask):\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mdpead/mdpead.github.io/translation/translation.ipynb#Y244sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msrc_embed(src), src_mask)\n",
      "File \u001b[0;32m~/mdpead.github.io/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mdpead.github.io/.venv/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/mdpead.github.io/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/mdpead/mdpead.github.io/translation/translation.ipynb Cell 134\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mdpead/mdpead.github.io/translation/translation.ipynb#Y244sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mdpead/mdpead.github.io/translation/translation.ipynb#Y244sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlut(x) \u001b[39m*\u001b[39m math\u001b[39m.\u001b[39msqrt(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_model)\n",
      "File \u001b[0;32m~/mdpead.github.io/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mdpead.github.io/.venv/lib/python3.11/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/mdpead.github.io/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2179\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2118\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"A simple lookup table that looks up embeddings in a fixed dictionary and size.\u001b[39;00m\n\u001b[1;32m   2119\u001b[0m \n\u001b[1;32m   2120\u001b[0m \u001b[39mThis module is often used to retrieve word embeddings using indices.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2175\u001b[0m \u001b[39m             [ 0.6262,  0.2438,  0.7471]]])\u001b[39;00m\n\u001b[1;32m   2176\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2178\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight):\n\u001b[0;32m-> 2179\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2180\u001b[0m         embedding,\n\u001b[1;32m   2181\u001b[0m         (\u001b[39minput\u001b[39;49m, weight),\n\u001b[1;32m   2182\u001b[0m         \u001b[39minput\u001b[39;49m,\n\u001b[1;32m   2183\u001b[0m         weight,\n\u001b[1;32m   2184\u001b[0m         padding_idx\u001b[39m=\u001b[39;49mpadding_idx,\n\u001b[1;32m   2185\u001b[0m         max_norm\u001b[39m=\u001b[39;49mmax_norm,\n\u001b[1;32m   2186\u001b[0m         norm_type\u001b[39m=\u001b[39;49mnorm_type,\n\u001b[1;32m   2187\u001b[0m         scale_grad_by_freq\u001b[39m=\u001b[39;49mscale_grad_by_freq,\n\u001b[1;32m   2188\u001b[0m         sparse\u001b[39m=\u001b[39;49msparse,\n\u001b[1;32m   2189\u001b[0m     )\n\u001b[1;32m   2190\u001b[0m \u001b[39mif\u001b[39;00m padding_idx \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2191\u001b[0m     \u001b[39mif\u001b[39;00m padding_idx \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/mdpead.github.io/.venv/lib/python3.11/site-packages/torch/overrides.py:1534\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m \u001b[39mif\u001b[39;00m _is_torch_function_mode_enabled():\n\u001b[1;32m   1531\u001b[0m     \u001b[39m# if we're here, the mode must be set to a TorchFunctionStackMode\u001b[39;00m\n\u001b[1;32m   1532\u001b[0m     \u001b[39m# this unsets it and calls directly into TorchFunctionStackMode's torch function\u001b[39;00m\n\u001b[1;32m   1533\u001b[0m     \u001b[39mwith\u001b[39;00m _pop_mode_temporarily() \u001b[39mas\u001b[39;00m mode:\n\u001b[0;32m-> 1534\u001b[0m         result \u001b[39m=\u001b[39m mode\u001b[39m.\u001b[39;49m__torch_function__(public_api, types, args, kwargs)\n\u001b[1;32m   1535\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[1;32m   1536\u001b[0m         \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/mdpead.github.io/.venv/lib/python3.11/site-packages/torch/utils/_device.py:62\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39mif\u001b[39;00m func \u001b[39min\u001b[39;00m _device_constructors() \u001b[39mand\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\n\u001b[0;32m---> 62\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/mdpead.github.io/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "run_epoch(\n",
    "    data_iter=train_dataloader,\n",
    "    model=example_model,\n",
    "    loss_compute=SimpleLossCompute(example_model.generator, LabelSmoothing(\n",
    "        size=VOCAB_SIZE, padding_idx=2, smoothing=0.1\n",
    "    )),\n",
    "    optimizer=example_optimizer,\n",
    "    scheduler=example_lr_scheduler,\n",
    "    mode=\"train\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_worker(\n",
    "    gpu,\n",
    "    config\n",
    "):\n",
    "    print(f\"Train worker process using GPU: {gpu} for training\", flush=True)\n",
    "    torch.cuda.device(gpu)\n",
    "\n",
    "    pad_idx = 2\n",
    "    d_model = 512\n",
    "    model = make_model(VOCAB_SIZE, VOCAB_SIZE, N=6)\n",
    "    model.cuda(gpu)\n",
    "    module = model\n",
    "    is_main_process = True\n",
    "\n",
    "    criterion = LabelSmoothing(\n",
    "        size=VOCAB_SIZE, padding_idx=pad_idx, smoothing=0.1\n",
    "    )\n",
    "    criterion.cuda(gpu)\n",
    "\n",
    "    # train_dataloader, valid_dataloader = create_dataloaders(\n",
    "    #     gpu,\n",
    "    #     vocab_src,\n",
    "    #     vocab_tgt,\n",
    "    #     spacy_de,\n",
    "    #     spacy_en,\n",
    "    #     batch_size=config[\"batch_size\"] // ngpus_per_node,\n",
    "    #     max_padding=config[\"max_padding\"],\n",
    "    #     is_distributed=is_distributed,\n",
    "    # )\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=config[\"base_lr\"], betas=(0.9, 0.98), eps=1e-9\n",
    "    )\n",
    "    lr_scheduler = LambdaLR(\n",
    "        optimizer=optimizer,\n",
    "        lr_lambda=lambda step: rate(\n",
    "            step, d_model, factor=1, warmup=config[\"warmup\"]\n",
    "        ),\n",
    "    )\n",
    "    train_state = TrainState()\n",
    "\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        model.train()\n",
    "        print(f\"[GPU{gpu}] Epoch {epoch} Training ====\", flush=True)\n",
    "        _, train_state = run_epoch(\n",
    "            train_dataloader,\n",
    "            model,\n",
    "            SimpleLossCompute(module.generator, criterion),\n",
    "            optimizer,\n",
    "            lr_scheduler,\n",
    "            mode=\"train+log\",\n",
    "            accum_iter=config[\"accum_iter\"],\n",
    "            train_state=train_state,\n",
    "        )\n",
    "\n",
    "        #GPUtil.showUtilization()\n",
    "        if is_main_process:\n",
    "            file_path = \"%s%.2d.pt\" % (config[\"file_prefix\"], epoch)\n",
    "            torch.save(module.state_dict(), file_path)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        print(f\"[GPU{gpu}] Epoch {epoch} Validation ====\", flush=True)\n",
    "        model.eval()\n",
    "        sloss = run_epoch(\n",
    "            val_dataloader,\n",
    "            model,\n",
    "            SimpleLossCompute(module.generator, criterion),\n",
    "            DummyOptimizer(),\n",
    "            DummyScheduler(),\n",
    "            mode=\"eval\",\n",
    "        )\n",
    "        print(sloss)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    if is_main_process:\n",
    "        file_path = \"%sfinal.pt\" % config[\"file_prefix\"]\n",
    "        torch.save(module.state_dict(), file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"batch_size\": 32,\n",
    "    \"distributed\": False,\n",
    "    \"num_epochs\": 1,\n",
    "    \"accum_iter\": 10,\n",
    "    \"base_lr\": 1.0,\n",
    "    \"max_padding\": 72,\n",
    "    \"warmup\": 3000,\n",
    "    \"file_prefix\": \"english_welsh_model_\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train worker process using GPU: -1 for training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1307/320853634.py:20: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(p)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Device index must not be negative",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/mdpead/mdpead.github.io/translation/translation.ipynb Cell 139\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mdpead/mdpead.github.io/translation/translation.ipynb#Y254sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m train_worker(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, config)\n",
      "\u001b[1;32m/home/mdpead/mdpead.github.io/translation/translation.ipynb Cell 139\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mdpead/mdpead.github.io/translation/translation.ipynb#Y254sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m d_model \u001b[39m=\u001b[39m \u001b[39m512\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mdpead/mdpead.github.io/translation/translation.ipynb#Y254sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m model \u001b[39m=\u001b[39m make_model(VOCAB_SIZE, VOCAB_SIZE, N\u001b[39m=\u001b[39m\u001b[39m6\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mdpead/mdpead.github.io/translation/translation.ipynb#Y254sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m model\u001b[39m.\u001b[39;49mcuda(gpu)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mdpead/mdpead.github.io/translation/translation.ipynb#Y254sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m module \u001b[39m=\u001b[39m model\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mdpead/mdpead.github.io/translation/translation.ipynb#Y254sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m is_main_process \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/mdpead.github.io/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:905\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcuda\u001b[39m(\u001b[39mself\u001b[39m: T, device: Optional[Union[\u001b[39mint\u001b[39m, device]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m    889\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    890\u001b[0m \n\u001b[1;32m    891\u001b[0m \u001b[39m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[1;32m    904\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 905\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(\u001b[39mlambda\u001b[39;49;00m t: t\u001b[39m.\u001b[39;49mcuda(device))\n",
      "File \u001b[0;32m~/mdpead.github.io/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/mdpead.github.io/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 797 (3 times)]\u001b[0m\n",
      "File \u001b[0;32m~/mdpead.github.io/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/mdpead.github.io/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/mdpead.github.io/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:905\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcuda\u001b[39m(\u001b[39mself\u001b[39m: T, device: Optional[Union[\u001b[39mint\u001b[39m, device]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m    889\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    890\u001b[0m \n\u001b[1;32m    891\u001b[0m \u001b[39m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[1;32m    904\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 905\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply(\u001b[39mlambda\u001b[39;00m t: t\u001b[39m.\u001b[39;49mcuda(device))\n",
      "File \u001b[0;32m~/mdpead.github.io/.venv/lib/python3.11/site-packages/torch/utils/_device.py:62\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39mif\u001b[39;00m func \u001b[39min\u001b[39;00m _device_constructors() \u001b[39mand\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\n\u001b[0;32m---> 62\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Device index must not be negative"
     ]
    }
   ],
   "source": [
    "train_worker(-1, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model built and trained, we can use it to translate sentences in our test corpus from English to Welsh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    memory = model.encode(src, src_mask)\n",
    "    tgt = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    for i in range(max_len-1):\n",
    "        out = model.decode(memory = memory, src_mask = src_mask,\n",
    "                           tgt = Variable(tgt),\n",
    "                           tgt_mask = Variable(subsequent_mask(tgt.size(1))\n",
    "                                    .type_as(src.data)))\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim = 1)\n",
    "        next_word = next_word.data[0]\n",
    "        tgt = torch.cat([tgt,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "    return tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   0,   45,  741,  131,  131,  167,  813, 2058,  112,  671, 8549,  435,\n",
       "         5596, 1245,   18,    1,    2,    2,    2,    2,    2,    2],\n",
       "        [   0, 4402, 1068,  250, 2665,  398,  202,  697, 2062,  768,   16,  136,\n",
       "          112, 7402,  170,  583, 9528,   83,  246, 1028,   18,    1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tokenizer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[BOS] i hope that that will help create the right atmosphere when answering questions. [EOS] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]']\n",
      "[\"[BOS] gobeithiaf y bydd hynny o gymorth i greu'r awyrgylch iawn wrth ateb cwestiynau. [EOS] [PAD] [PAD] [PAD] [PAD]\"]\n",
      "['[BOS] paragraff gofynnodd oaq20 ddirbertbert rheoli ebrillymod rogers rogers rogers rogers rogers rogers rogers rogers rogers rogers']\n"
     ]
    }
   ],
   "source": [
    "model = example_model\n",
    "\n",
    "src = example_batch['src_input_ids'][0:1, ]\n",
    "src_mask = example_batch['src_attention_mask'][0:1, ]\n",
    "max_len=20\n",
    "start_symbol=0\n",
    "\n",
    "pred_tokens = greedy_decode(model, src, src_mask, max_len, start_symbol)\n",
    "pred_text = welsh_tokenizer.batch_decode(pred_tokens)\n",
    "actual_text = welsh_tokenizer.batch_decode(example_batch['tgt_input_ids'][0:1, ])\n",
    "print(english_tokenizer.batch_decode(example_batch['src_input_ids'][0:1, ]))\n",
    "print(actual_text)\n",
    "print(pred_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[BOS] when we read that, we thought that it meant that the formula would be reviewed upwards, not downwards as has been the case in the last two years. [EOS] [PAD] [PAD] [PAD] [PAD]']\n",
      "['[BOS] pan ddarllenasom hynny, yr oeddem yn meddwl ei fod yn golygu y cai ’ r fformiwla ei adolygu tuag i fyny, nid tuag i lawr fel sydd wedi digwydd yn y ddwy flynedd diwethaf.']\n",
      "['[BOS]efyd hwyr fis funud gynhyrchu camarwain pethau funud dren cambri disgybl ymgysylltu edifar hoffi ieith cambri gohirio disgyblellach']\n"
     ]
    }
   ],
   "source": [
    "test_example = collate_batch(corpus['test'][1000:1001])\n",
    "pred_tokens = greedy_decode(example_model, test_example['src_input_ids'], test_example['src_attention_mask'], max_len=20, start_symbol=0)\n",
    "pred_text = welsh_tokenizer.batch_decode(pred_tokens)\n",
    "actual_text = welsh_tokenizer.batch_decode(test_example['tgt_input_ids'][0:1, ])\n",
    "print(english_tokenizer.batch_decode(test_example['src_input_ids'][0:1, ]))\n",
    "print(actual_text)\n",
    "print(pred_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
